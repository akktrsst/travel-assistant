# Hugging Face API Token - Required for using Hugging Face's Inference API
# You'll need to get this from https://huggingface.co/settings/tokens
HF_API_TOKEN=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

# Model Configuration
DEFAULT_MODEL=mistralai/Mistral-7B-Instruct-v0.1
MAX_NEW_TOKENS=512
TEMPERATURE=0.7

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
DEBUG_MODE=True

# Local Model Path (optional, for local model deployment)
LOCAL_MODEL_PATH=models/llama-2-7b-chat.gguf

# Rate Limiting (requests per minute)
RATE_LIMIT=60 